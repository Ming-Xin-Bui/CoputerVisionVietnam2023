{"cells":[{"cell_type":"code","execution_count":35,"metadata":{"id":"KuERH9acBP8I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688934487793,"user_tz":-120,"elapsed":16981,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}},"outputId":"59bd3d95-4ea2-4019-b0eb-b2260c166879"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","fatal: destination path 'ConZIC' already exists and is not an empty directory.\n","/content/ConZIC\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (6.7.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (4.30.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.15.2+cu118)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (0.3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (4.6.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 4)) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 4)) (16.0.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 5)) (8.4.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 3)) (2023.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 4)) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 4)) (1.3.0)\n"]}],"source":["#@title Prepare the running enviroment\n","import os\n","%cd /content\n","!git clone https://github.com/joeyz0z/ConZIC.git\n","%cd /content/ConZIC\n","!pip install -r requirements.txt\n","\n","!pip install flask --quiet\n","!pip install flask-ngrok --quiet\n","# install ngrok linux version using the following command or you can get the\n","# latest version from its official website- https://dashboard.ngrok.com/get-started/setup\n","if not os.path.isfile(\"/content/ngrok-stable-linux-amd64.tgz\"):\n","  !wget -P /content/ https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n","  print(\"Downloaded ngrok!\")"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AF1FIK0Qlpc","executionInfo":{"status":"ok","timestamp":1688934491340,"user_tz":-120,"elapsed":3568,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}},"outputId":"aa91a463-4921-42a0-e370-1c8bfce167b8"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["### set-up\n","\n","# extract the downloaded file using the following command\n","!tar -xvf /content/ngrok-stable-linux-amd64.tgz\n","# paste your AuthToken here and execute this command\n","!./ngrok authtoken 2SKzeGEvuO8zabsgzKqwCQRZinc_3vzWUp5Ek9uU3PP9Fuz3G\n","\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('sentiwordnet')\n","nltk.download('universal_tagset')\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# add path to your static and template folder for pictures and html/css/js here. see flask folder structure.\n","TEMPLATE = '/content/gdrive/MyDrive/Colab Notebooks/VnCv2023/templates'\n","STATIC = '/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVdwP9h66Ue6","executionInfo":{"status":"ok","timestamp":1688934495355,"user_tz":-120,"elapsed":4025,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}},"outputId":"1ffb28f7-b3b7-4beb-a550-3a4f1b7bdcd5"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["ngrok\n","Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n","[nltk_data]   Package sentiwordnet is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# flask for deployment\n","from flask import Flask, render_template, request\n","from flask_ngrok import run_with_ngrok\n","app = Flask(__name__, template_folder=TEMPLATE, static_folder=STATIC)\n","run_with_ngrok(app)\n","\n","@app.route(\"/\")\n","def home():\n","    return render_template(\"home.html\")\n"],"metadata":{"id":"pa1TIW7l0mFW","executionInfo":{"status":"ok","timestamp":1688934495356,"user_tz":-120,"elapsed":39,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","execution_count":39,"metadata":{"id":"NWgr_QNCZQzt","executionInfo":{"status":"ok","timestamp":1688934495356,"user_tz":-120,"elapsed":38,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"outputs":[],"source":["#@title Upload your image or use our examples\n","# upload_your_image = False # @param {type:\"boolean\"}"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"kr0jcnIQ6tNs","executionInfo":{"status":"ok","timestamp":1688934495357,"user_tz":-120,"elapsed":38,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e84dd32-2e21-4b54-b10c-c11d4497c677"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/ConZIC/examples/cat.png\n"]}],"source":["import os\n","#@title Select examples\n","example_name = 'cat.png' # @param ['cat.png', 'girl.jpg', 'Gosh.jpeg', 'horse.png']\n","example_img_path = os.path.join(os.getcwd(), 'examples', example_name)\n","print(example_img_path)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"6jOxjjNq7DWQ","executionInfo":{"status":"ok","timestamp":1688934495357,"user_tz":-120,"elapsed":34,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"outputs":[],"source":["#@title Upload image\n","\n","import os\n","from werkzeug.utils import secure_filename\n","\n","\n","# !mkdir your_uploaded_image\n","# %cd /content/ConZIC/your_uploaded_image\n","# uploaded = files.upload()\n","upload_img_path = ''\n","img_name = ''\n","@app.route('/uploader', methods=['GET', 'POST'])\n","def upload_file():\n","    if request.method == 'POST':\n","        f = request.files['fileBtn']\n","        if f.filename == '':\n","            return render_template(\"home.html\", message=\"Please choose a picture\")\n","\n","        app.config['UPLOAD_FOLDER'] = os.path.join(STATIC, 'pictures')\n","\n","        f.save(os.path.join(app.config['UPLOAD_FOLDER'], secure_filename(f.filename)))\n","        global img_name\n","        img_name = f.filename\n","        global upload_img_path\n","        upload_img_path = os.path.join(app.config['UPLOAD_FOLDER'], img_name)\n","        print(upload_img_path)\n","\n","        return render_template(\"Uploaded.html\")\n","\n","\n","\n","\n","# if not uploaded:\n","#   img_name = ''\n","# elif len(uploaded) == 1:\n","#   img_name = list(uploaded.keys())[0]\n","# else:\n","#   raise AssertionError('Please upload one image at a time')\n","\n","# upload_img_path = os.path.join(os.getcwd(), img_name)\n","# print(upload_img_path)\n","# %cd /content/ConZIC"]},{"cell_type":"code","source":[],"metadata":{"id":"AQ-_X5cusaKX","executionInfo":{"status":"ok","timestamp":1688934495358,"user_tz":-120,"elapsed":34,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","execution_count":42,"metadata":{"id":"y2EA-wkCiwSk","executionInfo":{"status":"ok","timestamp":1688934495358,"user_tz":-120,"elapsed":34,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"outputs":[],"source":["# @title Import\n","import sys\n","sys.path.append('/content/ConZIC')\n","\n","from utils import create_logger,set_seed\n","import os\n","import time\n","import argparse\n","import json\n","from PIL import Image\n","import torch\n","\n","from clip.clip import CLIP\n","from gen_utils import generate_caption\n","from control_gen_utils import control_generate_caption\n","from transformers import AutoModelForMaskedLM, AutoTokenizer"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"K-rdVFhFDc1y","executionInfo":{"status":"ok","timestamp":1688934495358,"user_tz":-120,"elapsed":33,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"outputs":[],"source":["# @title Define parameters\n","def get_args():\n","    parser = argparse.ArgumentParser()\n","\n","    parser.add_argument(\"--seed\", type=int, default=42)\n","    parser.add_argument(\"--batch_size\", type=int, default=1, help = \"Only supports batch_size=1 currently.\")\n","    parser.add_argument(\"--device\", type=str,\n","                        default='cuda',choices=['cuda','cpu'])\n","\n","    ## Generation and Controllable Type\n","    parser.add_argument('--run_type',\n","                        default='caption',\n","                        nargs='?',\n","                        choices=['caption', 'controllable'])\n","    parser.add_argument('--prompt',\n","                        default='Image of a',type=str)\n","    parser.add_argument('--order',\n","                        default='shuffle',\n","                        nargs='?',\n","                        choices=['sequential', 'shuffle', 'span', 'random','parallel'],\n","                        help=\"Generation order of text\")\n","    parser.add_argument('--control_type',\n","                        default='sentiment',\n","                        nargs='?',\n","                        choices=[\"sentiment\",\"pos\"],\n","                        help=\"which controllable task to conduct\")\n","    parser.add_argument('--pos_type', type=list,\n","                        default=[['DET'], ['ADJ','NOUN'], ['NOUN'],\n","                                 ['VERB'], ['VERB'],['ADV'], ['ADP'],\n","                                 ['DET','NOUN'], ['NOUN'], ['NOUN','.'],\n","                                 ['.','NOUN'],['.','NOUN']],\n","                        help=\"predefined part-of-speech templete\")\n","    parser.add_argument('--sentiment_type',\n","                        default=\"positive\",\n","                        nargs='?',\n","                        choices=[\"positive\", \"negative\"])\n","    parser.add_argument('--samples_num',\n","                        default=2,type=int) # Nghĩa là số  sample(caption) được gen ra từ mô hình.\n","\n","    ## Hyperparameters\n","    parser.add_argument(\"--sentence_len\", type=int, default=10)\n","    parser.add_argument(\"--candidate_k\", type=int, default=200)\n","    parser.add_argument(\"--alpha\", type=float, default=0.02, help=\"weight for fluency\")\n","    parser.add_argument(\"--beta\", type=float, default=2.0, help=\"weight for image-matching degree\")\n","    parser.add_argument(\"--gamma\", type=float, default=5.0, help=\"weight for controllable degree\")\n","    parser.add_argument(\"--lm_temperature\", type=float, default=0.1)\n","    parser.add_argument(\"--num_iterations\", type=int, default=10, help=\"predefined iterations for Gibbs Sampling\")\n","\n","    ## Models and Paths\n","    parser.add_argument(\"--lm_model\", type=str, default='bert-base-uncased',\n","                        help=\"Path to language model\") # bert,roberta\n","    parser.add_argument(\"--match_model\", type=str, default='openai/clip-vit-base-patch32',\n","                        help=\"Path to Image-Text model\")  # clip,align\n","    parser.add_argument(\"--caption_img_path\", type=str, default='./examples/girl.jpg',\n","                        help=\"file path of the image for captioning\")\n","    parser.add_argument(\"--stop_words_path\", type=str, default='stop_words.txt',\n","                        help=\"Path to stop_words.txt\")\n","    parser.add_argument(\"--add_extra_stopwords\", type=list, default=[],\n","                        help=\"you can add some extra stop words\")\n","\n","    args = parser.parse_args(args=[])\n","    return args"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"y75YazZuIM5Z","executionInfo":{"status":"ok","timestamp":1688934495359,"user_tz":-120,"elapsed":33,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"outputs":[],"source":["# @title Image captioning\n","import functools\n","\n","def run_caption(args, image_path, lm_model, lm_tokenizer, clip, token_mask, logger):\n","    FinalCaptionList = []\n","    BestCaptionList = []\n","    logger.info(f\"Processing: {image_path}\")\n","    image_instance = Image.open(image_path).convert(\"RGB\")\n","    # img_name = [image_path.spilt(\"/\")[-1]]\n","    for sample_id in range(args.samples_num):\n","        logger.info(f\"Sample {sample_id}: \")\n","        gen_texts, clip_scores = generate_caption(img_name, lm_model, clip, lm_tokenizer, image_instance, token_mask, logger,\n","                                  prompt=args.prompt, batch_size=args.batch_size, max_len=args.sentence_len,\n","                                  top_k=args.candidate_k, temperature=args.lm_temperature,\n","                                  max_iter=args.num_iterations,alpha=args.alpha,beta=args.beta,\n","                                  generate_order = args.order)\n","        str1 = ''\n","        for i in gen_texts[-2]:\n","            str1 += i\n","        str2 = ''\n","        for i in gen_texts[-1]:\n","            str2 += i\n","\n","        FinalCaptionStr = \"Sample {}: \".format(sample_id + 1) + str1\n","        BestCaptionStr = \"Sample {}: \".format(sample_id + 1) + str2\n","        FinalCaptionList.append(FinalCaptionStr)\n","        BestCaptionList.append(BestCaptionStr)\n","    return FinalCaptionList, BestCaptionList\n","\n","\n","def run_control(run_type, args, image_path, lm_model, lm_tokenizer, clip, token_mask, logger):\n","    FinalCaptionList = []\n","    BestCaptionList = []\n","    logger.info(f\"Processing: {image_path}\")\n","    image_instance = Image.open(image_path).convert(\"RGB\")\n","    for sample_id in range(args.samples_num):\n","        logger.info(f\"Sample {sample_id}: \")\n","        gen_texts, clip_scores = control_generate_caption(img_name, lm_model, clip, lm_tokenizer, image_instance, token_mask, logger,\n","                                  prompt=args.prompt, batch_size=args.batch_size, max_len=args.sentence_len,\n","                                  top_k=args.candidate_k, temperature=args.lm_temperature,\n","                                  max_iter=args.num_iterations, alpha=args.alpha,\n","                                  beta=args.beta, gamma=args.gamma,\n","                                  ctl_type = args.control_type, style_type=args.sentiment_type,pos_type=args.pos_type, generate_order=args.order)\n","        print(\"Test for debugging: \", len(gen_texts))\n","        gen_texts = functools.reduce(lambda x,y: x+y,gen_texts)\n","        # Chuyển gentexts từ dạng hierachy list thành dạng list chỉ 1 string, không bao gồm các list con.\n","        str1 = ''\n","        for i in gen_texts[-2]:\n","            str1 += i\n","        str2 = ''\n","        for i in gen_texts[-1]:\n","            str2 += i\n","\n","        FinalCaptionStr = \"Sample {}: \".format(sample_id + 1) + str1\n","\n","        BestCaptionStr = \"Sample {}: \".format(sample_id + 1) + str2\n","        FinalCaptionList.append(FinalCaptionStr)\n","        BestCaptionList.append(BestCaptionStr)\n","    return FinalCaptionList, BestCaptionList\n"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"7tptbdD15WFJ","executionInfo":{"status":"ok","timestamp":1688934495359,"user_tz":-120,"elapsed":32,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"outputs":[],"source":["# @title GPU/CPU\n","is_gpu = True #@param {type:\"boolean\"}"]},{"cell_type":"markdown","metadata":{"id":"u8tYBMBF5WFJ"},"source":["\n","\n","*   RunType: Select RunType equal to \"contrallable\" to control text generation.\n","*   ControlType: Control text by sentiment or part of speech.\n","*   SentimentType: Control sentiment: positive or negative\n","*   Order: Generation order of text\n","*   Alpha: Weight for fluency; Choose between 0 and 1\n","*   Beta: Weight for image-matching degree; Choose between 1 and 5\n","*   Gamma: Weight for controllable degree; Choose between 1 and 10\n","*   SampleNum: Number of runs; Choose between 1 and 5\n","*   Length: Sentence length; Choose between 5 and 10\n","*   NumIterations: Iterations for Gibbs Sampling; Choose between 1 and 15"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"b11C9FxI5WFK","executionInfo":{"status":"ok","timestamp":1688934495359,"user_tz":-120,"elapsed":32,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"outputs":[],"source":["args = get_args()\n","\n","@app.route('/configure', methods=['GET', 'POST'])\n","def configure():\n","    if request.method == 'POST':\n","        is_gpu = int(request.form.get('isGpu'))\n","        args.sentence_len = int(request.form.get('length'))\n","        args.run_type = request.form.get('runType')\n","        args.control_type = request.form.get('controlType')\n","        args.sentiment_type = request.form.get('sentimentType')\n","        args.alpha = float(request.form.get('alpha'))\n","        args.beta = float(request.form.get('beta'))\n","        args.gamma = float(request.form.get('gamma'))\n","        args.samples_num = int(request.form.get('samplesNum'))\n","        args.order = request.form.get('order')\n","        args.num_iterations = int(request.form.get('numIterations'))\n","        args.caption_img_path = upload_img_path\n","        args.device = \"cuda\" if is_gpu else \"cpu\"\n","        set_seed(args.seed)\n","        return render_template(\"processing.html\")\n","\n","\n","# @title Select types and parameters\n","# args = get_args()\n","\n","# RunType = 'controllable' # @param ['caption', 'controllable']\n","# ControlType = 'sentiment' # @param [\"sentiment\",\"pos\"]\n","# SentimentType = 'positive' # @param [\"positive\", \"negative\"]\n","# Order = 'shuffle' # @param ['sequential', 'shuffle', 'random']\n","# Alpha = 0.08 # @param {type:\"slider\", min:0, max:1, step:0.01}\n","# Beta = 2 # @param {type:\"slider\", min:1, max:5, step:0.5}\n","# Gamma = 5 # @param {type:\"slider\", min:1, max:10, step:0.5}\n","# SamplesNum = 2 # @param {type:\"slider\", min:1, max:5, step:1}\n","# Length = 15 # @param {type:\"slider\", min:5, max:15,  step:1}\n","# NumIterations = 5 # @param {type:\"slider\", min:1, max:15, step:1}\n","\n","# args.sentence_len = Length\n","# args.run_type = RunType\n","# args.control_type = ControlType\n","# args.sentiment_type = SentimentType\n","# args.alpha = Alpha\n","# args.beta = Beta\n","# args.gamma = Gamma\n","# args.samples_num = SamplesNum\n","# args.order = Order\n","# args.num_iterations = NumIterations\n","# args.caption_img_path = upload_img_path\n","# args.device = \"cuda\" if is_gpu else \"cpu\"\n"," #set_seed(args.seed)"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"GzTnVb7_5WFL","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1688934495360,"user_tz":-120,"elapsed":32,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}},"outputId":"bd94113e-59e3-4959-bd3e-115c3972cd57"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nrun_type = \"caption\" if args.run_type==\"caption\" else args.control_type\\nif run_type==\"sentiment\":\\n    run_type = args.sentiment_type\\n\\nif os.path.exists(\"logger\")== False:\\n    os.mkdir(\"logger\")\\nlogger = create_logger(\\n    \"logger\",\\'demo_{}_{}_len{}_topk{}_alpha{}_beta{}_gamma{}_lmtemp{}_{}.log\\'.format(\\n    run_type, args.order, args.sentence_len,\\n    args.candidate_k, args.alpha, args.beta,args.gamma, args.lm_temperature,\\n    time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())))\\n\\nlogger.info(f\"Generating order:{args.order}\")\\nlogger.info(f\"Run type:{run_type}\")\\nlogger.info(args)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}],"source":["# @title Creat logger\n","\n","@app.route('/processing')\n","def processing():\n","    run_type = \"caption\" if args.run_type == \"caption\" else args.control_type\n","    if run_type == \"sentiment\":\n","        run_type = args.sentiment_type\n","\n","    if not os.path.exists(\"/content/ConZIC/logger\"):\n","        os.mkdir(\"/content/ConZIC/logger\")\n","    logger = create_logger(\n","        \"/content/ConZIC/logger/\", 'demo_{}_{}_len{}_topk{}_alpha{}_beta{}_gamma{}_lmtemp{}_{}.log'.format(\n","            run_type, args.order, args.sentence_len,\n","            args.candidate_k, args.alpha, args.beta, args.gamma, args.lm_temperature,\n","            time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())))\n","\n","    logger.info(f\"Generating order:{args.order}\")\n","    logger.info(f\"Run type:{run_type}\")\n","    logger.info(args)\n","\n","    # @title Load pre-trained model (weights)\n","    lm_model = AutoModelForMaskedLM.from_pretrained(args.lm_model)\n","    lm_tokenizer = AutoTokenizer.from_pretrained(args.lm_model)\n","    lm_model.eval()\n","    clip = CLIP(args.match_model)\n","    clip.eval()\n","    lm_model = lm_model.to(args.device)\n","    clip = clip.to(args.device)\n","\n","    # @title Remove stop words, token mask\n","    with open(args.stop_words_path, 'r', encoding='utf-8') as stop_words_file:\n","        stop_words = stop_words_file.readlines()\n","        stop_words_ = [stop_word.rstrip('\\n') for stop_word in stop_words]\n","        stop_words_ += args.add_extra_stopwords\n","        stop_ids = lm_tokenizer.convert_tokens_to_ids(stop_words_)\n","        token_mask = torch.ones((1, lm_tokenizer.vocab_size))\n","        for stop_id in stop_ids:\n","            token_mask[0, stop_id] = 0\n","        token_mask = token_mask.to(args.device)\n","\n","    # title Run\n","    img_path = upload_img_path  # if upload_your_image else example_img_path\n","    if args.run_type == 'caption':\n","        FinalCaption, BestCaption = run_caption(args, img_path, lm_model, lm_tokenizer, clip, token_mask, logger)\n","    elif args.run_type == 'controllable':\n","        FinalCaption, BestCaption = run_control(run_type, args, img_path, lm_model, lm_tokenizer, clip, token_mask,\n","                                                logger)\n","    else:\n","        raise Exception('run_type must be caption or controllable!')\n","\n","    # @title Output\n","    # Image.open(img_path).show()\n","    print(\"Final Caption\\n\")\n","    for i in range(len(FinalCaption)):\n","        print(f\"{FinalCaption[i]}\\n\")\n","    print(\"Best Caption\\n\")\n","    for i in range(len(BestCaption)):\n","        print(f\"{BestCaption[i]}\\n\")\n","\n","    return render_template(\"Result.html\", fc=FinalCaption[0], bc=BestCaption[0], image='pictures/' + img_name)\n","\n","\n","'''\n","run_type = \"caption\" if args.run_type==\"caption\" else args.control_type\n","if run_type==\"sentiment\":\n","    run_type = args.sentiment_type\n","\n","if os.path.exists(\"logger\")== False:\n","    os.mkdir(\"logger\")\n","logger = create_logger(\n","    \"logger\",'demo_{}_{}_len{}_topk{}_alpha{}_beta{}_gamma{}_lmtemp{}_{}.log'.format(\n","    run_type, args.order, args.sentence_len,\n","    args.candidate_k, args.alpha, args.beta,args.gamma, args.lm_temperature,\n","    time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())))\n","\n","logger.info(f\"Generating order:{args.order}\")\n","logger.info(f\"Run type:{run_type}\")\n","logger.info(args)\n","'''"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"0RMmUNsuL3at","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1688934495360,"user_tz":-120,"elapsed":30,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}},"outputId":"ac308a04-a7af-492f-c780-73ae8b4a7379"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nlm_model = AutoModelForMaskedLM.from_pretrained(args.lm_model)\\nlm_tokenizer = AutoTokenizer.from_pretrained(args.lm_model)\\nlm_model.eval()\\nclip = CLIP(args.match_model)\\nclip.eval()\\n\\nlm_model = lm_model.to(args.device)\\nclip = clip.to(args.device)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}],"source":["# @title Load pre-trained model (weights)\n","'''\n","lm_model = AutoModelForMaskedLM.from_pretrained(args.lm_model)\n","lm_tokenizer = AutoTokenizer.from_pretrained(args.lm_model)\n","lm_model.eval()\n","clip = CLIP(args.match_model)\n","clip.eval()\n","\n","lm_model = lm_model.to(args.device)\n","clip = clip.to(args.device)\n","'''"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"Sa7QUpE35WFL","executionInfo":{"status":"ok","timestamp":1688934495361,"user_tz":-120,"elapsed":30,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}},"colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"b0dcacdc-296a-4a8e-bedd-c4a94a4d7a94"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nwith open(args.stop_words_path,'r',encoding='utf-8') as stop_words_file:\\n    stop_words = stop_words_file.readlines()\\n    stop_words_ = [stop_word.rstrip('\\n') for stop_word in stop_words]\\n    stop_words_ += args.add_extra_stopwords\\n    stop_ids = lm_tokenizer.convert_tokens_to_ids(stop_words_)\\n    token_mask = torch.ones((1,lm_tokenizer.vocab_size))\\n    for stop_id in stop_ids:\\n        token_mask[0,stop_id]=0\\n    token_mask = token_mask.to(args.device)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}],"source":["#@title Remove stop words, token mask\n","\"\"\"\n","with open(args.stop_words_path,'r',encoding='utf-8') as stop_words_file:\n","    stop_words = stop_words_file.readlines()\n","    stop_words_ = [stop_word.rstrip('\\n') for stop_word in stop_words]\n","    stop_words_ += args.add_extra_stopwords\n","    stop_ids = lm_tokenizer.convert_tokens_to_ids(stop_words_)\n","    token_mask = torch.ones((1,lm_tokenizer.vocab_size))\n","    for stop_id in stop_ids:\n","        token_mask[0,stop_id]=0\n","    token_mask = token_mask.to(args.device)\n","\"\"\""]},{"cell_type":"code","execution_count":50,"metadata":{"id":"ahLYF7Q_5WFM","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"aa4d697a-3fe6-4e66-c761-d417324b4cdb","executionInfo":{"status":"ok","timestamp":1688934934075,"user_tz":-120,"elapsed":438742,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}}},"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:4040\n","DEBUG:urllib3.connectionpool:http://localhost:4040 \"GET /api/tunnels HTTP/1.1\" 200 803\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://eaa2-35-243-132-76.ngrok-free.app\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:28:55] \"GET / HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:28:56] \"GET /static/styles/home.css HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:28:56] \"GET /static/home.js HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:28:56] \"GET / HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:28:57] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:29:03] \"\u001b[31m\u001b[1mPOST /uploader HTTP/1.1\u001b[0m\" 400 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:29:05] \"POST /uploader HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:29:06] \"\u001b[36mGET /static/styles/home.css HTTP/1.1\u001b[0m\" 304 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:29:20] \"POST /configure HTTP/1.1\" 200 -\n","Generating order:random\u001b[0m\n","Generating order:random\u001b[0m\n","Generating order:random\u001b[0m\n","Generating order:random\u001b[0m\n","INFO:ConZIC:Generating order:random\n","Run type:pos\u001b[0m\n","Run type:pos\u001b[0m\n","Run type:pos\u001b[0m\n","Run type:pos\u001b[0m\n","INFO:ConZIC:Run type:pos\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='controllable', prompt='Image of a', order='random', control_type='pos', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='negative', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='controllable', prompt='Image of a', order='random', control_type='pos', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='negative', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='controllable', prompt='Image of a', order='random', control_type='pos', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='negative', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='controllable', prompt='Image of a', order='random', control_type='pos', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='negative', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","INFO:ConZIC:Namespace(seed=42, batch_size=1, device='cpu', run_type='controllable', prompt='Image of a', order='random', control_type='pos', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='negative', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\n","DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/generation_config.json HTTP/1.1\" 404 0\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch32/resolve/main/config.json HTTP/1.1\" 200 0\n"]},{"output_type":"stream","name":"stdout","text":["Initializing CLIP model...\n"]},{"output_type":"stream","name":"stderr","text":["DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch32/resolve/main/vocab.json HTTP/1.1\" 200 0\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG\u001b[0m\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG\u001b[0m\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG\u001b[0m\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG\u001b[0m\n","INFO:ConZIC:Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/AKJX1203.JPG\n","Sample 0: \u001b[0m\n","Sample 0: \u001b[0m\n","Sample 0: \u001b[0m\n","Sample 0: \u001b[0m\n","INFO:ConZIC:Sample 0: \n","[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']]\u001b[0m\n","[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']]\u001b[0m\n","[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']]\u001b[0m\n","[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']]\u001b[0m\n","INFO:ConZIC:[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']]\n"]},{"output_type":"stream","name":"stdout","text":["CLIP model initialized.\n","Cuda is not available.\n","Device is -1\n"]},{"output_type":"stream","name":"stderr","text":["iter 1, The 1-th image: A, clip score 0.223, ctl score 0.167: [CLS] image of a couple of the same people eating togethers beside a big red brick building. [SEP]\u001b[0m\n","iter 1, The 1-th image: A, clip score 0.223, ctl score 0.167: [CLS] image of a couple of the same people eating togethers beside a big red brick building. [SEP]\u001b[0m\n","iter 1, The 1-th image: A, clip score 0.223, ctl score 0.167: [CLS] image of a couple of the same people eating togethers beside a big red brick building. [SEP]\u001b[0m\n","iter 1, The 1-th image: A, clip score 0.223, ctl score 0.167: [CLS] image of a couple of the same people eating togethers beside a big red brick building. [SEP]\u001b[0m\n","INFO:ConZIC:iter 1, The 1-th image: A, clip score 0.223, ctl score 0.167: [CLS] image of a couple of the same people eating togethers beside a big red brick building. [SEP]\n","['NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN', '.']\u001b[0m\n","['NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN', '.']\u001b[0m\n","['NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN', '.']\u001b[0m\n","['NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN', '.']\u001b[0m\n","INFO:ConZIC:['NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN', '.']\n","Finished in 77.675s\u001b[0m\n","Finished in 77.675s\u001b[0m\n","Finished in 77.675s\u001b[0m\n","Finished in 77.675s\u001b[0m\n","INFO:ConZIC:Finished in 77.675s\n","The 1-th image: A\u001b[0m\n","The 1-th image: A\u001b[0m\n","The 1-th image: A\u001b[0m\n","The 1-th image: A\u001b[0m\n","INFO:ConZIC:The 1-th image: A\n","final caption: image of a couple of the same people eating togethers beside a big red brick building.\u001b[0m\n","final caption: image of a couple of the same people eating togethers beside a big red brick building.\u001b[0m\n","final caption: image of a couple of the same people eating togethers beside a big red brick building.\u001b[0m\n","final caption: image of a couple of the same people eating togethers beside a big red brick building.\u001b[0m\n","INFO:ConZIC:final caption: image of a couple of the same people eating togethers beside a big red brick building.\n","best caption: image of a couple of the same people eating togethers beside a big red brick building.\u001b[0m\n","best caption: image of a couple of the same people eating togethers beside a big red brick building.\u001b[0m\n","best caption: image of a couple of the same people eating togethers beside a big red brick building.\u001b[0m\n","best caption: image of a couple of the same people eating togethers beside a big red brick building.\u001b[0m\n","INFO:ConZIC:best caption: image of a couple of the same people eating togethers beside a big red brick building.\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:30:42] \"GET /processing HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["Test for debugging:  2\n","Final Caption\n","\n","Sample 1: image of a couple of the same people eating togethers beside a big red brick building.\n","\n","Best Caption\n","\n","Sample 1: image of a couple of the same people eating togethers beside a big red brick building.\n","\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:30:43] \"GET /static/pictures/AKJX1203.JPG HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:31:21] \"GET / HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:31:21] \"\u001b[36mGET /static/home.js HTTP/1.1\u001b[0m\" 304 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:31:21] \"\u001b[36mGET /static/styles/home.css HTTP/1.1\u001b[0m\" 304 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:31:22] \"GET / HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:31:25] \"\u001b[31m\u001b[1mPOST /uploader HTTP/1.1\u001b[0m\" 400 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:31:32] \"POST /uploader HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:31:33] \"\u001b[36mGET /static/styles/home.css HTTP/1.1\u001b[0m\" 304 -\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:32:13] \"POST /configure HTTP/1.1\" 200 -\n","Generating order:shuffle\u001b[0m\n","Generating order:shuffle\u001b[0m\n","Generating order:shuffle\u001b[0m\n","Generating order:shuffle\u001b[0m\n","Generating order:shuffle\u001b[0m\n","INFO:ConZIC:Generating order:shuffle\n","Run type:caption\u001b[0m\n","Run type:caption\u001b[0m\n","Run type:caption\u001b[0m\n","Run type:caption\u001b[0m\n","Run type:caption\u001b[0m\n","INFO:ConZIC:Run type:caption\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='caption', prompt='Image of a', order='shuffle', control_type='sentiment', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='postive', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='caption', prompt='Image of a', order='shuffle', control_type='sentiment', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='postive', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='caption', prompt='Image of a', order='shuffle', control_type='sentiment', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='postive', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='caption', prompt='Image of a', order='shuffle', control_type='sentiment', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='postive', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","Namespace(seed=42, batch_size=1, device='cpu', run_type='caption', prompt='Image of a', order='shuffle', control_type='sentiment', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='postive', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\u001b[0m\n","INFO:ConZIC:Namespace(seed=42, batch_size=1, device='cpu', run_type='caption', prompt='Image of a', order='shuffle', control_type='sentiment', pos_type=[['DET'], ['ADJ', 'NOUN'], ['NOUN'], ['VERB'], ['VERB'], ['ADV'], ['ADP'], ['DET', 'NOUN'], ['NOUN'], ['NOUN', '.'], ['.', 'NOUN'], ['.', 'NOUN']], sentiment_type='postive', samples_num=1, sentence_len=15, candidate_k=200, alpha=0.55, beta=2.5, gamma=5.0, lm_temperature=0.1, num_iterations=1, lm_model='bert-base-uncased', match_model='openai/clip-vit-base-patch32', caption_img_path='/content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG', stop_words_path='stop_words.txt', add_extra_stopwords=[])\n","DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/generation_config.json HTTP/1.1\" 404 0\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch32/resolve/main/config.json HTTP/1.1\" 200 0\n"]},{"output_type":"stream","name":"stdout","text":["Initializing CLIP model...\n"]},{"output_type":"stream","name":"stderr","text":["DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json HTTP/1.1\" 200 0\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n","DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch32/resolve/main/vocab.json HTTP/1.1\" 200 0\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG\u001b[0m\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG\u001b[0m\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG\u001b[0m\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG\u001b[0m\n","Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG\u001b[0m\n","INFO:ConZIC:Processing: /content/gdrive/My Drive/Colab Notebooks/VnCv2023/static/pictures/ARHV4034.JPG\n","Sample 0: \u001b[0m\n","Sample 0: \u001b[0m\n","Sample 0: \u001b[0m\n","Sample 0: \u001b[0m\n","Sample 0: \u001b[0m\n","INFO:ConZIC:Sample 0: \n"]},{"output_type":"stream","name":"stdout","text":["CLIP model initialized.\n","Cuda is not available.\n","Device is -1\n"]},{"output_type":"stream","name":"stderr","text":["Order_list:[8, 13, 7, 6, 14, 12, 5, 2, 9, 3, 4, 11, 0, 1, 10]\u001b[0m\n","Order_list:[8, 13, 7, 6, 14, 12, 5, 2, 9, 3, 4, 11, 0, 1, 10]\u001b[0m\n","Order_list:[8, 13, 7, 6, 14, 12, 5, 2, 9, 3, 4, 11, 0, 1, 10]\u001b[0m\n","Order_list:[8, 13, 7, 6, 14, 12, 5, 2, 9, 3, 4, 11, 0, 1, 10]\u001b[0m\n","Order_list:[8, 13, 7, 6, 14, 12, 5, 2, 9, 3, 4, 11, 0, 1, 10]\u001b[0m\n","INFO:ConZIC:Order_list:[8, 13, 7, 6, 14, 12, 5, 2, 9, 3, 4, 11, 0, 1, 10]\n","iter 1, The 1-th image: A,clip score 0.210: [CLS] image of a large format picture handwritten with a big e y r and e q. [SEP]\u001b[0m\n","iter 1, The 1-th image: A,clip score 0.210: [CLS] image of a large format picture handwritten with a big e y r and e q. [SEP]\u001b[0m\n","iter 1, The 1-th image: A,clip score 0.210: [CLS] image of a large format picture handwritten with a big e y r and e q. [SEP]\u001b[0m\n","iter 1, The 1-th image: A,clip score 0.210: [CLS] image of a large format picture handwritten with a big e y r and e q. [SEP]\u001b[0m\n","iter 1, The 1-th image: A,clip score 0.210: [CLS] image of a large format picture handwritten with a big e y r and e q. [SEP]\u001b[0m\n","INFO:ConZIC:iter 1, The 1-th image: A,clip score 0.210: [CLS] image of a large format picture handwritten with a big e y r and e q. [SEP]\n","Finished in 69.622s\u001b[0m\n","Finished in 69.622s\u001b[0m\n","Finished in 69.622s\u001b[0m\n","Finished in 69.622s\u001b[0m\n","Finished in 69.622s\u001b[0m\n","INFO:ConZIC:Finished in 69.622s\n","The 1-th image: A\u001b[0m\n","The 1-th image: A\u001b[0m\n","The 1-th image: A\u001b[0m\n","The 1-th image: A\u001b[0m\n","The 1-th image: A\u001b[0m\n","INFO:ConZIC:The 1-th image: A\n","final caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","final caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","final caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","final caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","final caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","INFO:ConZIC:final caption: image of a large format picture handwritten with a big e y r and e q.\n","best caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","best caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","best caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","best caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","best caption: image of a large format picture handwritten with a big e y r and e q.\u001b[0m\n","INFO:ConZIC:best caption: image of a large format picture handwritten with a big e y r and e q.\n","INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:33:27] \"GET /processing HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["Final Caption\n","\n","Sample 1: image of a large format picture handwritten with a big e y r and e q.\n","\n","Best Caption\n","\n","Sample 1: image of a large format picture handwritten with a big e y r and e q.\n","\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [09/Jul/2023 20:33:28] \"GET /static/pictures/ARHV4034.JPG HTTP/1.1\" 200 -\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\nimg_path = upload_img_path# if upload_your_image else example_img_path\\nif args.run_type == 'caption':\\n    FinalCaption, BestCaption = run_caption(args, img_path, lm_model, lm_tokenizer, clip, token_mask, logger)\\nelif args.run_type == 'controllable':\\n    FinalCaption, BestCaption = run_control(run_type, args, img_path, lm_model, lm_tokenizer, clip, token_mask, logger)\\nelse:\\n    raise Exception('run_type must be caption or controllable!')\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":50}],"source":["# @title Run\n","app.run()\n","\"\"\"\n","img_path = upload_img_path# if upload_your_image else example_img_path\n","if args.run_type == 'caption':\n","    FinalCaption, BestCaption = run_caption(args, img_path, lm_model, lm_tokenizer, clip, token_mask, logger)\n","elif args.run_type == 'controllable':\n","    FinalCaption, BestCaption = run_control(run_type, args, img_path, lm_model, lm_tokenizer, clip, token_mask, logger)\n","else:\n","    raise Exception('run_type must be caption or controllable!')\n","\"\"\""]},{"cell_type":"code","execution_count":51,"metadata":{"id":"oPrtlmIR5WFM","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1688934934078,"user_tz":-120,"elapsed":18,"user":{"displayName":"Mi Xi","userId":"14998189389836315456"}},"outputId":"04d701f5-336d-4350-968d-86acae3f2929"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# @title Output\\nImage.open(img_path).show()\\nprint(\"Final Caption\\n\")\\nfor i in range(len(FinalCaption)):\\n    print(f\"{FinalCaption[i]}\\n\")\\nprint(\"Best Caption\\n\")\\nfor i in range(len(BestCaption)):\\n    print(f\"{BestCaption[i]}\\n\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}],"source":["\"\"\"\n","# @title Output\n","Image.open(img_path).show()\n","print(\"Final Caption\\n\")\n","for i in range(len(FinalCaption)):\n","    print(f\"{FinalCaption[i]}\\n\")\n","print(\"Best Caption\\n\")\n","for i in range(len(BestCaption)):\n","    print(f\"{BestCaption[i]}\\n\")\n","\"\"\""]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1BiNlzNV3LkQY2W9qICOnlBSxwkxLnA-W","timestamp":1688909562211},{"file_id":"1MyjEAuygQblYwTjK67ATo7XALuHkkiLa","timestamp":1686929859960}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.15"}},"nbformat":4,"nbformat_minor":0}